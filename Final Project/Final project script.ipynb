{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACADEMY and TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded S3 key Talent/Sparta Day 1 August 2019.txt to local file data\\Sparta Day 1 August 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 1 May 2019.txt to local file data\\Sparta Day 1 May 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 1 October 2019.txt to local file data\\Sparta Day 1 October 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 10 April 2019.txt to local file data\\Sparta Day 10 April 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 10 December 2019.txt to local file data\\Sparta Day 10 December 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 10 January 2019.txt to local file data\\Sparta Day 10 January 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 10 July 2019.txt to local file data\\Sparta Day 10 July 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 10 October 2019.txt to local file data\\Sparta Day 10 October 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 10 September 2019.txt to local file data\\Sparta Day 10 September 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 11 April 2019.txt to local file data\\Sparta Day 11 April 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 11 December 2019.txt to local file data\\Sparta Day 11 December 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 11 July 2019.txt to local file data\\Sparta Day 11 July 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 11 June 2019.txt to local file data\\Sparta Day 11 June 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 11 September 2019.txt to local file data\\Sparta Day 11 September 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 12 December 2019.txt to local file data\\Sparta Day 12 December 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 12 February 2019.txt to local file data\\Sparta Day 12 February 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 12 June 2019.txt to local file data\\Sparta Day 12 June 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 12 March 2019.txt to local file data\\Sparta Day 12 March 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 12 November 2019.txt to local file data\\Sparta Day 12 November 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 12 September 2019.txt to local file data\\Sparta Day 12 September 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 13 August 2019.txt to local file data\\Sparta Day 13 August 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 13 February 2019.txt to local file data\\Sparta Day 13 February 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 13 June 2019.txt to local file data\\Sparta Day 13 June 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 13 March 2019.txt to local file data\\Sparta Day 13 March 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 13 November 2019.txt to local file data\\Sparta Day 13 November 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 14 August 2019.txt to local file data\\Sparta Day 14 August 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 14 February 2019.txt to local file data\\Sparta Day 14 February 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 14 March 2019.txt to local file data\\Sparta Day 14 March 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 14 May 2019.txt to local file data\\Sparta Day 14 May 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 14 November 2019.txt to local file data\\Sparta Day 14 November 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 15 August 2019.txt to local file data\\Sparta Day 15 August 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 15 January 2019.txt to local file data\\Sparta Day 15 January 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 15 May 2019.txt to local file data\\Sparta Day 15 May 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 15 October 2019.txt to local file data\\Sparta Day 15 October 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 16 April 2019.txt to local file data\\Sparta Day 16 April 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 16 January 2019.txt to local file data\\Sparta Day 16 January 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 16 July 2019.txt to local file data\\Sparta Day 16 July 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 16 May 2019.txt to local file data\\Sparta Day 16 May 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 16 October 2019.txt to local file data\\Sparta Day 16 October 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 17 April 2019.txt to local file data\\Sparta Day 17 April 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 17 December 2019.txt to local file data\\Sparta Day 17 December 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 17 January 2019.txt to local file data\\Sparta Day 17 January 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 17 July 2019.txt to local file data\\Sparta Day 17 July 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 17 October 2019.txt to local file data\\Sparta Day 17 October 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 17 September 2019.txt to local file data\\Sparta Day 17 September 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 18 April 2019.txt to local file data\\Sparta Day 18 April 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 18 December 2019.txt to local file data\\Sparta Day 18 December 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 18 July 2019.txt to local file data\\Sparta Day 18 July 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 18 June 2019.txt to local file data\\Sparta Day 18 June 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 18 September 2019.txt to local file data\\Sparta Day 18 September 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 19 December 2019.txt to local file data\\Sparta Day 19 December 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 19 February 2019.txt to local file data\\Sparta Day 19 February 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 19 June 2019.txt to local file data\\Sparta Day 19 June 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 19 March 2019.txt to local file data\\Sparta Day 19 March 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 19 November 2019.txt to local file data\\Sparta Day 19 November 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 19 September 2019.txt to local file data\\Sparta Day 19 September 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 2 April 2019.txt to local file data\\Sparta Day 2 April 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 2 January 2019.txt to local file data\\Sparta Day 2 January 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 2 July 2019.txt to local file data\\Sparta Day 2 July 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 2 May 2019.txt to local file data\\Sparta Day 2 May 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 2 October 2019.txt to local file data\\Sparta Day 2 October 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 20 August 2019.txt to local file data\\Sparta Day 20 August 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 20 February 2019.txt to local file data\\Sparta Day 20 February 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 20 June 2019.txt to local file data\\Sparta Day 20 June 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 20 March 2019.txt to local file data\\Sparta Day 20 March 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 20 November 2019.txt to local file data\\Sparta Day 20 November 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 21 August 2019.txt to local file data\\Sparta Day 21 August 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 21 February 2019.txt to local file data\\Sparta Day 21 February 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 21 March 2019.txt to local file data\\Sparta Day 21 March 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 21 May 2019.txt to local file data\\Sparta Day 21 May 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 21 November 2019.txt to local file data\\Sparta Day 21 November 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 22 August 2019.txt to local file data\\Sparta Day 22 August 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 22 January 2019.txt to local file data\\Sparta Day 22 January 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 22 May 2019.txt to local file data\\Sparta Day 22 May 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 22 October 2019.txt to local file data\\Sparta Day 22 October 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 23 April 2019.txt to local file data\\Sparta Day 23 April 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 23 January 2019.txt to local file data\\Sparta Day 23 January 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 23 July 2019.txt to local file data\\Sparta Day 23 July 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 23 May 2019.txt to local file data\\Sparta Day 23 May 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 23 October 2019.txt to local file data\\Sparta Day 23 October 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 24 April 2019.txt to local file data\\Sparta Day 24 April 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 24 January 2019.txt to local file data\\Sparta Day 24 January 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 24 July 2019.txt to local file data\\Sparta Day 24 July 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 24 October 2019.txt to local file data\\Sparta Day 24 October 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 24 September 2019.txt to local file data\\Sparta Day 24 September 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 25 April 2019.txt to local file data\\Sparta Day 25 April 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 25 July 2019.txt to local file data\\Sparta Day 25 July 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 25 June 2019.txt to local file data\\Sparta Day 25 June 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 25 September 2019.txt to local file data\\Sparta Day 25 September 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 26 February 2019.txt to local file data\\Sparta Day 26 February 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 26 June 2019.txt to local file data\\Sparta Day 26 June 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 26 March 2019.txt to local file data\\Sparta Day 26 March 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 26 November 2019.txt to local file data\\Sparta Day 26 November 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 26 September 2019.txt to local file data\\Sparta Day 26 September 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 27 August 2019.txt to local file data\\Sparta Day 27 August 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 27 February 2019.txt to local file data\\Sparta Day 27 February 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 27 June 2019.txt to local file data\\Sparta Day 27 June 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 27 March 2019.txt to local file data\\Sparta Day 27 March 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 27 November 2019.txt to local file data\\Sparta Day 27 November 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 28 August 2019.txt to local file data\\Sparta Day 28 August 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 28 February 2019.txt to local file data\\Sparta Day 28 February 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 28 March 2019.txt to local file data\\Sparta Day 28 March 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 28 May 2019.txt to local file data\\Sparta Day 28 May 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 28 November 2019.txt to local file data\\Sparta Day 28 November 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 29 August 2019.txt to local file data\\Sparta Day 29 August 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 29 January 2019.txt to local file data\\Sparta Day 29 January 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 29 May 2019.txt to local file data\\Sparta Day 29 May 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 29 October 2019.txt to local file data\\Sparta Day 29 October 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 3 April 2019.txt to local file data\\Sparta Day 3 April 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 3 December 2019.txt to local file data\\Sparta Day 3 December 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 3 January 2019.txt to local file data\\Sparta Day 3 January 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 3 July 2019.txt to local file data\\Sparta Day 3 July 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 3 October 2019.txt to local file data\\Sparta Day 3 October 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 3 September 2019.txt to local file data\\Sparta Day 3 September 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 30 April 2019.txt to local file data\\Sparta Day 30 April 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 30 January 2019.txt to local file data\\Sparta Day 30 January 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 30 July 2019.txt to local file data\\Sparta Day 30 July 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 30 May 2019.txt to local file data\\Sparta Day 30 May 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 30 October 2019.txt to local file data\\Sparta Day 30 October 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 31 January 2019.txt to local file data\\Sparta Day 31 January 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 31 July 2019.txt to local file data\\Sparta Day 31 July 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 31 October 2019.txt to local file data\\Sparta Day 31 October 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 4 April 2019.txt to local file data\\Sparta Day 4 April 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 4 December 2019.txt to local file data\\Sparta Day 4 December 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 4 July 2019.txt to local file data\\Sparta Day 4 July 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 4 June 2019.txt to local file data\\Sparta Day 4 June 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 4 September 2019.txt to local file data\\Sparta Day 4 September 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 5 December 2019.txt to local file data\\Sparta Day 5 December 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 5 February 2019.txt to local file data\\Sparta Day 5 February 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 5 June 2019.txt to local file data\\Sparta Day 5 June 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 5 March 2019.txt to local file data\\Sparta Day 5 March 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 5 November 2019.txt to local file data\\Sparta Day 5 November 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 5 September 2019.txt to local file data\\Sparta Day 5 September 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 6 August 2019.txt to local file data\\Sparta Day 6 August 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 6 February 2019.txt to local file data\\Sparta Day 6 February 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 6 June 2019.txt to local file data\\Sparta Day 6 June 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 6 March 2019.txt to local file data\\Sparta Day 6 March 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 6 November 2019.txt to local file data\\Sparta Day 6 November 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 7 August 2019.txt to local file data\\Sparta Day 7 August 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 7 February 2019.txt to local file data\\Sparta Day 7 February 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 7 March 2019.txt to local file data\\Sparta Day 7 March 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 7 May 2019.txt to local file data\\Sparta Day 7 May 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 7 November 2019.txt to local file data\\Sparta Day 7 November 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 8 August 2019.txt to local file data\\Sparta Day 8 August 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 8 January 2019.txt to local file data\\Sparta Day 8 January 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 8 May 2019.txt to local file data\\Sparta Day 8 May 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 8 October 2019.txt to local file data\\Sparta Day 8 October 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 9 April 2019.txt to local file data\\Sparta Day 9 April 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 9 January 2019.txt to local file data\\Sparta Day 9 January 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 9 July 2019.txt to local file data\\Sparta Day 9 July 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 9 May 2019.txt to local file data\\Sparta Day 9 May 2019.txt\n",
      "Downloaded S3 key Talent/Sparta Day 9 October 2019.txt to local file data\\Sparta Day 9 October 2019.txt\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime as dt\n",
    "from io import BytesIO, StringIO\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "\n",
    "def get_s3_objects(bucket_name, folder_path):\n",
    "    s3_client = boto3.client('s3')\n",
    "    objects = s3_client.list_objects(Bucket=bucket_name, Prefix=folder_path)\n",
    "    keys = [obj['Key'] for obj in objects['Contents'] if obj['Key']]\n",
    "    dfs = []\n",
    "    for key in keys:\n",
    "        obj = s3_client.get_object(Bucket=bucket_name, Key=key)\n",
    "        df = pd.read_csv(obj['Body'], delimiter=',')\n",
    "        df['filename'] = key.split('/')[-1]\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "file_type = '.txt'\n",
    "prefix='Talent/'\n",
    "bucket_name = 'data-eng-204-final-project'\n",
    "\n",
    "# Use paginator to retrieve all objects in the bucket with the given prefix\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
    "# Download each .txt file from S3\n",
    "for page in page_iterator:\n",
    "    for obj in page['Contents']:\n",
    "        key = obj['Key']\n",
    "        if key.endswith(file_type):\n",
    "            local_file = os.path.join(f'data', os.path.basename(key))\n",
    "            s3.download_file(bucket_name, key, local_file)\n",
    "            print(f\"Downloaded S3 key {key} to local file {local_file}\")\n",
    "\n",
    "\n",
    "def get_course_dataframe(bucket_name, course_name):\n",
    "    folder_path = f'Academy/{course_name}'\n",
    "    df = get_s3_objects(bucket_name, folder_path)\n",
    "    df['course'] = df['filename'].str.extract(r'^(.*?)_')\n",
    "    df['started_date'] = df['filename'].str.extract(r'_(\\d{4}-\\d{2}-\\d{2})\\.csv$')\n",
    "    cols = df.columns.tolist()\n",
    "    cols.remove('trainer')\n",
    "    cols.insert(1, 'trainer')\n",
    "    cols.remove('course')\n",
    "    cols.insert(2, 'course')\n",
    "    cols.remove('started_date')\n",
    "    cols.insert(3, 'started_date')\n",
    "    df = df.reindex(columns=cols)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_all_courses_dataframe(bucket_name):\n",
    "    df_business = get_course_dataframe(bucket_name, 'Business')\n",
    "    df_data = get_course_dataframe(bucket_name, 'Data')\n",
    "    df_eng = get_course_dataframe(bucket_name, 'Engineering')\n",
    "    return pd.concat([df_business, df_data, df_eng], ignore_index=True)\n",
    "\n",
    "\n",
    "def normalise_scores_df(bucket_name):\n",
    "    all_courses = get_all_courses_dataframe(bucket_name)\n",
    "    weeks = [f'W{i}' for i in range(1, 11)]\n",
    "    traits = ['Analytic', 'Independent', 'Determined', 'Professional', 'Studious', 'Imaginative']\n",
    "    id_vars = ['name', 'trainer', 'course', 'started_date']\n",
    "    value_vars = [f'{trait}_{week}' for trait in traits for week in weeks]\n",
    "    scores_df = pd.melt(all_courses, id_vars=id_vars, value_vars=value_vars, var_name='trait_week', value_name='score')\n",
    "    scores_df[['trait', 'week']] = scores_df['trait_week'].str.split('_W', expand=True)\n",
    "    scores_df.drop('trait_week', axis=1, inplace=True)\n",
    "    return scores_df\n",
    "\n",
    "\n",
    "# CLEANS THE TEXT DATAFRAMES \n",
    "\n",
    "def text_cleaner(file_name):\n",
    "    file = open(file_name,'r')\n",
    "    topology_list = file.readlines()\n",
    "    formats = \"%d %B %Y\"\n",
    "\n",
    "    my_dict = {\n",
    "        'name': [],\n",
    "        'date': [],\n",
    "        'academy': [],\n",
    "        'presentation': [],\n",
    "        'pyschometrics': []\n",
    "    }\n",
    "\n",
    "    for i, c in enumerate(topology_list):\n",
    "        if i == 0:\n",
    "            date = str(c[:-1])\n",
    "            date = date.replace(\"Monday \", \"\").replace(\"Tuesday \", '').replace(\"Wednesday \", '').replace(\"Thursday \", '').replace(\"Friday \", '')\n",
    "            date_cleaned = dt.datetime.strptime(date, formats)\n",
    "        elif i == 1:\n",
    "            academy = str(c[:-9])\n",
    "        elif i >= 3:\n",
    "            pres_score = int(c[-6:-4])\n",
    "            my_dict['presentation'].append(pres_score)\n",
    "            psych_score = int(c[-28:-26])\n",
    "            psych_p = psych_score\n",
    "            my_dict['pyschometrics'].append(psych_score)\n",
    "            name = str(c[0:-47])\n",
    "            my_dict['name'].append(name)\n",
    "            my_dict['academy'].append(academy)\n",
    "            my_dict['date'].append(date_cleaned)\n",
    "    return pd.DataFrame(my_dict)\n",
    "\n",
    "normalised_df = normalise_scores_df(bucket_name='data-eng-204-final-project')\n",
    "\n",
    "normalised_df['trainer'].unique()\n",
    "\n",
    "normalised_df['trainer'] = normalised_df['trainer'].replace('Ely Kely', 'Elly Kelly')\n",
    "\n",
    "normalised_df['name'] = normalised_df['name'].str.upper()\n",
    "\n",
    "normalised_df['started_date'] = pd.to_datetime(normalised_df['started_date'])\n",
    "normalised_df['week'] = pd.to_numeric(normalised_df['week'])\n",
    "\n",
    "temp = []\n",
    "# ITERATE THROUGH THE FILES AND PERFORM THE CLEANING (TXT)\n",
    "pathlist = Path('data').rglob('*')\n",
    "for path in pathlist:\n",
    "     path_in_str = str(path)\n",
    "     temp.append(text_cleaner(path_in_str))\n",
    "    #  print(path_in_str)\n",
    "\n",
    "df_txt = pd.concat(temp)\n",
    "\n",
    "normalised_df.to_csv('academy_clean.csv', index=False)\n",
    "\n",
    "df_txt.to_csv('txt_clean.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   name        date  \\\n",
      "0     Stillmann Castano  22/08/2019   \n",
      "1       Hilary Willmore  01/08/2019   \n",
      "2         Efrem Whipple  22/08/2019   \n",
      "3           Sydel Fenne  28/08/2019   \n",
      "4       Michel Lebarree  07/08/2019   \n",
      "...                 ...         ...   \n",
      "3100    Jacky Reilingen  04/04/2019   \n",
      "3101    Phillis Lyfield  10/04/2019   \n",
      "3102       Celle Barlas  16/04/2019   \n",
      "3103         Scott Duny  11/04/2019   \n",
      "3104  Boycey Matushenko  25/04/2019   \n",
      "\n",
      "                                        tech_self_score  \\\n",
      "0         {'C#': 6, 'Java': 5, 'R': 2, 'JavaScript': 2}   \n",
      "1           {'Python': 1, 'C#': 4, 'Java': 2, 'C++': 4}   \n",
      "2                                 {'Ruby': 4, 'C++': 4}   \n",
      "3                                {'Java': 3, 'SPSS': 4}   \n",
      "4     {'Python': 3, 'Java': 4, 'Ruby': 1, 'R': 2, 'P...   \n",
      "...                                                 ...   \n",
      "3100            {'C#': 2, 'Java': 6, 'R': 1, 'SPSS': 4}   \n",
      "3101          {'C#': 4, 'Java': 4, 'Ruby': 4, 'PHP': 1}   \n",
      "3102                {'R': 2, 'C++': 1, 'JavaScript': 4}   \n",
      "3103                                        {'Ruby': 3}   \n",
      "3104        {'Python': 2, 'C#': 3, 'Java': 3, 'C++': 4}   \n",
      "\n",
      "                                       strengths  \\\n",
      "0                                     [Charisma]   \n",
      "1            [Patient, Curious, Problem Solving]   \n",
      "2              [Courteous, Independent, Patient]   \n",
      "3                                   [Passionate]   \n",
      "4                                    [Versatile]   \n",
      "...                                          ...   \n",
      "3100                                 [Versatile]   \n",
      "3101     [Organisation, Independent, Determined]   \n",
      "3102                           [Problem Solving]   \n",
      "3103  [Reliable, Perfectionism, Problem Solving]   \n",
      "3104                               [Independent]   \n",
      "\n",
      "                                  weaknesses self_development geo_flex  \\\n",
      "0       [Distracted, Impulsive, Introverted]              Yes      Yes   \n",
      "1         [Overbearing, Chatty, Indifferent]               No      Yes   \n",
      "2          [Introverted, Impulsive, Anxious]              Yes      Yes   \n",
      "3                 [Perfectionist, Sensitive]              Yes      Yes   \n",
      "4       [Controlling, Perfectionist, Chatty]              Yes      Yes   \n",
      "...                                      ...              ...      ...   \n",
      "3100  [Indifferent, Intolerant, Introverted]              Yes       No   \n",
      "3101     [Sensitive, Overbearing, Impatient]              Yes      Yes   \n",
      "3102                              [Critical]              Yes      Yes   \n",
      "3103                              [Stubborn]              Yes      Yes   \n",
      "3104                 [Controlling, Stubborn]              Yes      Yes   \n",
      "\n",
      "     financial_support_self result course_interest  \n",
      "0                       Yes   Pass        Business  \n",
      "1                       Yes   Fail            Data  \n",
      "2                       Yes   Pass        Business  \n",
      "3                       Yes   Pass            Data  \n",
      "4                       Yes   Pass     Engineering  \n",
      "...                     ...    ...             ...  \n",
      "3100                    Yes   Fail        Business  \n",
      "3101                    Yes   Pass     Engineering  \n",
      "3102                    Yes   Pass     Engineering  \n",
      "3103                    Yes   Pass            Data  \n",
      "3104                     No   Fail     Engineering  \n",
      "\n",
      "[3105 rows x 10 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3105 entries, 0 to 3104\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   name                    3105 non-null   object\n",
      " 1   date                    3105 non-null   object\n",
      " 2   tech_self_score         3050 non-null   object\n",
      " 3   strengths               3105 non-null   object\n",
      " 4   weaknesses              3105 non-null   object\n",
      " 5   self_development        3105 non-null   object\n",
      " 6   geo_flex                3105 non-null   object\n",
      " 7   financial_support_self  3105 non-null   object\n",
      " 8   result                  3105 non-null   object\n",
      " 9   course_interest         3105 non-null   object\n",
      "dtypes: object(10)\n",
      "memory usage: 266.8+ KB\n"
     ]
    }
   ],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "s3_resource = boto3.resource('s3')\n",
    "session = boto3.session.Session()\n",
    "\n",
    "# Boto3 clients\n",
    "s3_client = boto3.client('s3')\n",
    "s3_resource = boto3.resource('s3')\n",
    "session = boto3.session.Session()\n",
    "#bucket_list = s3_client.list_buckets()\n",
    "\n",
    "\n",
    "bucket_name = 'data-eng-204-final-project'\n",
    "key = 'Talent/10383.json'\n",
    "\n",
    "obj_1 = s3_client.get_object(Bucket=bucket_name, Key=key)\n",
    "\n",
    "strbody = obj_1['Body'].read()\n",
    "body = json.loads(strbody)\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "list_of_dict = []\n",
    "\n",
    "paginator = s3_client.get_paginator('list_objects_v2')\n",
    "results = paginator.paginate(Bucket=bucket_name)\n",
    "for page in results:\n",
    "    if \"Contents\" in page:\n",
    "        for key in page[ \"Contents\" ]:\n",
    "            keyString = key[ \"Key\" ]\n",
    "            if '.json' in keyString:\n",
    "                obj_1 = s3_client.get_object(Bucket=bucket_name, Key=keyString)\n",
    "                strbody = obj_1['Body'].read()\n",
    "                body = json.loads(strbody)\n",
    "                list_of_dict.append(body)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(list_of_dict)\n",
    "df.to_csv('export_dataframe.csv')\n",
    "\n",
    "print(df)\n",
    "bucket_list = s3_client.list_buckets()\n",
    "bucket_name = 'data-eng-204-final-project'\n",
    "key = 'Talent/10383.json'\n",
    "\n",
    "obj_1 = s3_client.get_object(Bucket=bucket_name, Key=key)\n",
    "\n",
    "df = pd.read_csv(\"export_dataframe.csv\", index_col=0)\n",
    "df.info()\n",
    "\n",
    "df1 = df[df.isna().any(axis=1)]\n",
    "\n",
    "df['tech_self_score'].fillna('N/A',inplace=True)\n",
    "\n",
    "df[df['tech_self_score']=='N/A']\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df['date'], dayfirst=True)\n",
    "df['name'] = df['name'].str.upper()\n",
    "\n",
    "df.to_csv(r'clean_json.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TALENT INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully merged and saved 12 CSV files into 'merged_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "# Set up S3 client\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "# Set up S3 bucket and folder\n",
    "bucket_name = 'data-eng-204-final-project'\n",
    "folder_name = 'Talent/'\n",
    "\n",
    "# Get list of CSV files in folder\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "csv_files = [obj.key for obj in bucket.objects.filter(Prefix=folder_name) if obj.key.endswith('.csv')]\n",
    "\n",
    "# Merge all CSV files into a single DataFrame\n",
    "df_list = []\n",
    "for csv_file in csv_files:\n",
    "    obj = s3.Object(bucket_name, csv_file)\n",
    "    body = obj.get()['Body']\n",
    "    df = pd.read_csv(body)\n",
    "    df_list.append(df)\n",
    "merged_df = pd.concat(df_list)\n",
    "\n",
    "print(f\"Successfully merged and saved {len(df_list)} CSV files into 'merged_data.csv'.\")\n",
    "\n",
    "merged_df['name'] = merged_df['name'].str.upper()\n",
    "\n",
    "merged_df.fillna(value='unknown', inplace=True)\n",
    "\n",
    "merged_df.drop(columns=['address','uni','degree','invited_date','month','invited_by'],inplace=True)\n",
    "\n",
    "merged_df.drop(columns=['id'],inplace=True)\n",
    "\n",
    "merged_df.to_csv('talent_info.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\faizm\\AppData\\Local\\Temp\\ipykernel_1740\\2549756319.py:11: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df_talent['phone_number'] = df_talent['phone_number'].str.replace(' ', '-').str.replace('(', '').str.replace(')', '')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('talent_info.csv')\n",
    "df1 = pd.read_csv('txt_clean.csv')\n",
    "df2 = pd.read_csv('clean_json.csv')\n",
    "\n",
    "df_talent = pd.merge(pd.merge(df,df1, on='name', how='outer'), df2, on='name', how='outer')\n",
    "\n",
    "df_talent.drop('date_y', axis=1, inplace=True)\n",
    "\n",
    "df_talent.drop_duplicates(inplace=True)\n",
    "\n",
    "df_talent['phone_number'] = df_talent['phone_number'].str.replace(' ', '-').str.replace('(', '').str.replace(')', '')\n",
    "\n",
    "df_talent = df_talent.rename(columns={'pyschometrics': 'psychometrics', 'date_x': 'date'})\n",
    "\n",
    "df_talent['dob'] = df_talent['dob'].replace('unknown', None)\n",
    "\n",
    "df_talent['dob'] = pd.to_datetime(df_talent['dob'], format='%d/%m/%Y')\n",
    "df_talent['date'] = pd.to_datetime(df_talent['date'])\n",
    "\n",
    "df_talent.to_csv('talent_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded clean_json.csv to S3 bucket data-eng-204-final-project with key cleaned_data/clean_json.csv\n",
      "Uploaded academy_clean.csv to S3 bucket data-eng-204-final-project with key cleaned_data/academy_clean.csv\n",
      "Uploaded talent_combined.csv to S3 bucket data-eng-204-final-project with key cleaned_data/talent_combined.csv\n",
      "Uploaded talent_info.csv to S3 bucket data-eng-204-final-project with key cleaned_data/talent_info.csv\n",
      "Uploaded txt_clean.csv to S3 bucket data-eng-204-final-project with key cleaned_data/txt_clean.csv\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket_name = 'data-eng-204-final-project'\n",
    "prefix = 'cleaned_data/'\n",
    "\n",
    "# List of local files to upload\n",
    "local_files = ['clean_json.csv', 'academy_clean.csv', 'talent_combined.csv', 'talent_info.csv', 'txt_clean.csv']\n",
    "\n",
    "for file in local_files:\n",
    "    # Generate S3 key based on the file name\n",
    "    s3_key = prefix + file.split('/')[-1]\n",
    "\n",
    "    # Upload the file to S3\n",
    "    s3.upload_file(file, bucket_name, s3_key)\n",
    "\n",
    "    print(f\"Uploaded {file} to S3 bucket {bucket_name} with key {s3_key}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
